{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Install Dependencies\n",
    "First, install the required dependencies from `notebook-requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r notebook-requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ AF3 - System Settings\n",
    "These settings configure partitions, memory, CPU, and default timeouts. Below are the detailed system settings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datapipeline Partition (C3DH)\n",
    "- **Name**: {{ datapipeline_c3dhm_partition_name }}\n",
    "- **Machine Type**: {{ datapipeline_c3dhm_partition_machine_type }}\n",
    "- **Memory**: {{ datapipeline_c3dhm_partition_memory }} GB\n",
    "- **CPU Count**: {{ datapipeline_c3dhm_partition_cpu_count }}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Partition (G2)\n",
    "- **Name**: {{ inference_g2_partition_name }}\n",
    "- **Machine Type**: {{ inference_g2_partition_machine_type }}\n",
    "- **Memory**: {{ inference_g2_partition_memory }} GB\n",
    "- **CPU Count**: {{ inference_g2_partition_cpu_count }}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Partition (A2)\n",
    "- **Name**: {{ inference_a2_partition_name }}\n",
    "- **Machine Type**: {{ inference_a2_partition_machine_type }}\n",
    "- **Memory**: {{ inference_a2_partition_memory }} GB\n",
    "- **CPU Count**: {{ inference_a2_partition_cpu_count }}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Partition (A2U)\n",
    "- **Name**: {{ inference_a2u_partition_name }}\n",
    "- **Machine Type**: {{ inference_a2u_partition_machine_type }}\n",
    "- **Memory**: {{ inference_a2u_partition_memory }} GB\n",
    "- **CPU Count**: {{ inference_a2u_partition_cpu_count }}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ AF3 - Science Settings\n",
    "Scientific parameters for model behavior like seeds, iterations, and templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_template_date = \"{{ max_template_date }}\"\n",
    "conformer_max_iterations = \"{{ conformer_max_iterations }}\"\n",
    "num_recycles = \"{{ num_recycles }}\"\n",
    "num_diffusion_samples = \"{{ num_diffusion_samples }}\"\n",
    "num_seeds = \"{{ num_seeds }}\"\n",
    "save_embeddings = \"{{ save_embeddings }}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÇ Loading SLURM Info\n",
    "We will attempt to load the SLURM configuration data from a JSON file (`slurm_info.json`). If the file is missing, an error will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {}\n",
    "try:\n",
    "    with open(\"slurm_info.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        print(data)\n",
    "except FileNotFoundError:\n",
    "    print(\"Can't find slurm info file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîë AF3 - SLURM REST API\n",
    "This section contains the SLURM REST API configuration. The API is used to interact with the SLURM job scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Secret Name\n",
    "gcp_secret_name = \"{{ secret_name }}\"\n",
    "# Af3 default config\n",
    "af3_config = {\n",
    "    \"datapipeline_partition\": \"{{ default_datapipeline_partition_name }}\",\n",
    "    \"inference_partition\": \"{{ default_inference_partition_name }}\",\n",
    "    \"datapipeline_memory\": \"{{ default_datapipeline_memory }}\",\n",
    "    \"inference_memory\": \"{{ default_inference_memory }}\",\n",
    "    \"datapipeline_cpu_count\": \"{{ default_datapipeline_cpu_count }}\",\n",
    "    \"inference_cpu_count\": \"{{ default_inference_cpu_count }}\",\n",
    "    \"datapipeline_timeout\": \"{{ default_datapipeline_timeout }}\",\n",
    "    \"inference_timeout\": \"{{ default_inference_timeout }}\",\n",
    "    \"jax_compilation_cache_path\": \"{{ jax_compilation_cache_path }}\",\n",
    "    \"max_template_date\": max_template_date,\n",
    "    \"conformer_max_iterations\": conformer_max_iterations,\n",
    "    \"num_recycles\": num_recycles,\n",
    "    \"num_diffusion_samples\": num_diffusion_samples,\n",
    "    \"num_seeds\": num_seeds,\n",
    "    \"save_embeddings\": save_embeddings,\n",
    "    \n",
    "    \n",
    "    # Warning: don't change manually\n",
    "    \"sif_dir\": \"{{ sif_dir }}\",\n",
    "    \"model_dir\": \"{{ model_dir }}\",\n",
    "    \"db_dir\": \"{{ db_dir }}\",\n",
    "    \"pdb_database_path\": \"{{ pdb_database_path }}\",\n",
    "    \"default_folder\": \"{{ slurm_node_notebook_bucket_local_mount }}\",\n",
    "    \"job_template_path\": \"{{ af3_job_template_jupyter_path }}\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AF3SlurmClient\n",
    "from slurm_client import AF3SlurmClient\n",
    "client = AF3SlurmClient(remote_host=data[\"hostname\"],remote_port={{ api_port }}, gcp_project_id=\"{{ project_id }}\", gcp_secret_name=gcp_secret_name, af3_config=af3_config)\n",
    "\n",
    "# Example Usage: Ping\n",
    "ping_response = client.ping()\n",
    "print(\"Ping Response:\")\n",
    "print(json.dumps(ping_response,indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36dc9c1",
   "metadata": {},
   "source": [
    "## Example Usage: Submit Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Job configuration\n",
    "job_config = {\n",
    "    \"account\": \"{{ service_user }}\",\n",
    "    \"tasks\": 1,\n",
    "    \"name\": \"af3_slurm_rest_api_job\",\n",
    "    \"partition\": \"{{ default_datapipeline_partition_name }}\",\n",
    "    \"current_working_directory\": \"{{ working_directory }}\",\n",
    "    \"environment\": [\n",
    "        \"PATH=/bin:/usr/bin/:/usr/local/bin/\",\n",
    "        \"LD_LIBRARY_PATH=/lib/:/lib64/:/usr/local/lib\",\n",
    "    ],\n",
    "}\n",
    "script_command = \"#!/bin/bash\\nsleep 15\"\n",
    "\n",
    "# Send job config and its command \n",
    "submit_job_response = client.submit_job(job_config, script_command)\n",
    "print(\"Submit Job Response:\", submit_job_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßÆ Datapipeline\n",
    "Run Datapipeline from uploaded files. This is a simple example of how to run a Datapipeline job using the AF3 SLURM REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use SlurmClient to submit datapipeline job\n",
    "# Job configuration\n",
    "job_config = {\n",
    "    \"account\": \"{{ service_user }}\",\n",
    "    \"tasks\": 1,\n",
    "    \"name\": \"af3_slurm_rest_api_job\",\n",
    "    \"partition\": \"{{ default_datapipeline_partition_name }}\",\n",
    "    \"current_working_directory\": \"{{ working_directory }}\",\n",
    "    \"time_limit\": \"{{ default_datapipeline_timeout }}\",\n",
    "    \"memory_per_node\": \"{{ default_datapipeline_memory }}G\",\n",
    "    \"cpus_per_task\": {{ default_datapipeline_cpu_count }},\n",
    "    \"environment\": [\n",
    "        \"PATH=/bin:/usr/bin/:/usr/local/bin/\",\n",
    "        \"LD_LIBRARY_PATH=/lib/:/lib64/:/usr/local/lib\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Submit job\n",
    "# Assign `input_file` with datapipeline input file path\n",
    "input_file= \"\"\n",
    "submit_job_response = client.submit_data_pipeline_job(job_config, input_file)\n",
    "print(\"Submit Job Response:\", json.dumps(submit_job_response,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = 1 # Replace with the actual job ID you want to check\n",
    "# Check job status\n",
    "job_status_response = client.get_job_info(job_id)\n",
    "print(\"Job Status Response:\", json.dumps(job_status_response,indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Inference\n",
    "Run inference jobs using the AF3 SLURM REST API. This section demonstrates how to configure and submit an inference job through Slurm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use SlurmClient to submit inference job\n",
    "# Job configuration\n",
    "inference_job_config = {\n",
    "    \"account\": \"{{ service_user }}\",\n",
    "    \"tasks\": 1,\n",
    "    \"name\": \"af3_slurm_rest_api_inference_job\",\n",
    "    \"partition\": \"{{ default_inference_partition_name }}\",\n",
    "    \"current_working_directory\": \"{{ working_directory }}\",\n",
    "    \"time_limit\": \"{{ default_inference_timeout }}\",\n",
    "    \"memory_per_node\": \"{{ default_inference_memory }}G\",\n",
    "    \"cpus_per_task\": {{ default_inference_cpu_count }},\n",
    "    \"tres_per_job\": \"gres/gpu:1\", # Enables GPU\n",
    "    \"environment\": [\n",
    "        \"PATH=/bin:/usr/bin/:/usr/local/bin/\",\n",
    "        \"LD_LIBRARY_PATH=/lib/:/lib64/:/usr/local/lib\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Submit job\n",
    "# Assign `input_file` with inference input file path\n",
    "input_file = \"\"\n",
    "submit_inference_response = client.submit_inference_job(inference_job_config, input_file)\n",
    "print(\"Submit Inference Job Response:\", json.dumps(submit_inference_response,indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a910ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = 1 # Replace with the actual job ID you want to check\n",
    "# Check job status\n",
    "job_status_response = client.get_job_info(job_id)\n",
    "print(\"Job Status Response:\", json.dumps(job_status_response,indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b232147",
   "metadata": {},
   "source": [
    "# üß¨ Structure & PAE Viewer\n",
    "Run visualization from inference files. This is a simple example of how to run a visualization from inference result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b767809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "from Bio import PDB\n",
    "import py3Dmol\n",
    "from IPython.display import display\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def plot_pae_matrix(pae):\n",
    "    df = pd.DataFrame(pae)\n",
    "    fig = px.imshow(df, color_continuous_scale='Viridis',\n",
    "                    labels={'color': 'PAE'},\n",
    "                    title='Predicted Aligned Error (PAE) Matrix')\n",
    "    fig.update_layout(autosize=True)\n",
    "    fig.show()\n",
    "\n",
    "def read_cif_file(file_path):\n",
    "    parser = PDB.MMCIFParser(QUIET=True)\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    file_like = io.StringIO(content)\n",
    "    structure = parser.get_structure('protein', file_like)\n",
    "    return structure, content\n",
    "\n",
    "def extract_pae_from_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    pae = np.array(data.get('pae', []), dtype=np.float16)\n",
    "    token_chain_ids = data.get('token_chain_ids', [])\n",
    "    return pae, token_chain_ids\n",
    "\n",
    "\n",
    "def show_structure_3d(cif_string, width=500, height=400):\n",
    "    viewer = py3Dmol.view(width=width, height=height)\n",
    "    viewer.addModel(cif_string, 'cif')\n",
    "    viewer.setStyle({'cartoon': {'color': 'spectrum'}})\n",
    "    viewer.zoomTo()\n",
    "    viewer.show()\n",
    "    display(viewer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3025691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIF file\n",
    "# Note: Adjust the file path to your actual CIF file\n",
    "cif_file_path = \"\"\n",
    "structure, cif_content = read_cif_file(cif_file_path)\n",
    "show_structure_3d(cif_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa67b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and plot PAE matrix\n",
    "# Note: Adjust the file path to your actual PAE JSON file\n",
    "pae_json_file_path = \"\"  \n",
    "pae, chains = extract_pae_from_json(pae_json_file_path)\n",
    "plot_pae_matrix(pae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
